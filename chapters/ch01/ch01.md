Chapter 1

What is machine learning

Machine learning is everywhere. You book a flight ticket and it decides the price you are going to pay for the ticket. You apply for a loan and it may decide whether you are going to get it or not. You scroll through your Facebook timeline and it picks what advertisements to show to you. It also plays role in your Google search results. It organizes your email's inbox and filters out spam. It goes through your resume before the recruiters do when applying for a job. And finally, it is playing the role of your personal assistance in the form of Siri et al.

But how does machines actually learn? To be able to answer this question, let's take the following example of a fictional company. Space Shuttle Corporation has few space vehicles to rent. They get applications everyday from clients who want to rent their vehicles to Mars. They are not sure if those clients will return the vehicles back, or maybe they will decide to continue living on Mars and never come back again Even worse, some of the clients may be lousy pilots and destroy their vehicles. The company then decided to hire shuttle rent approval officers; their job is to go through applications and decide who is worthy a shuttle ride. Their business however grows too big that they need to automate part of the shuttle approval job. Traditionally, the shuttle company would start by having business rules, and hire junior employees to just execute those rules. If you are an alien, then sorry, you cannot rent a shuttle from us. If you are a human and you have kids in schools on earth, then you are okay to rent one of our shuttles. As you can see, those rules are too coarse. What about aliens who love living on earth and just want to go to mars for a quick holiday? To deal with that, the company starts to hire analysts. Their job is to go through historic data and try to come up with detailed rules, or business logic. The analysis can come up with very detailed rules. If you are an alien, one of your parents is from Neptune, your age is between 0.1 and 0.2 Neptunian years and you have 3 to 4 kids one of them is 80% or more human then you are allowed to rent a shuttle. To be able to come up with correct rules the analysts also need a way to measure how good the business logic is. Say, what percentage of the shuttles would return if certain rules were applied. They use the historic data to evaluate those measures, and only then we can say that those rules are actually learned from data.

Machine learning works almost the same way. You want to use historic data to come up with some business logic (algorithm) in order to optimize some measure of how good the logic is (objective or loss function). Throughout this book, we will learn about numerous machine learning algorithms, they differ from each other in how they represent the business logic, what objective functions they use, and what optimization techniques they utilize to reach a model that maximizes (or sometimes minimizes or) the objective function. Like the analysts in the previous example, you should pick an objective function that is as close as possible to your business objective. Anytime you hear people saying data scientist should have good understanding of their business, a significant part of that is their choice of a good objective function and ways to evaluate their model. In my example, I quickly picked the percentage of shuttles returned as my objective. But if you think about it, is it a 1-to-1 mapping to the shuttles company's revenue? Is the revenue made by allowing a trip equal to the cost of losing a shuttle? Furthermore, rejecting a trip may also cost your company angry calls to the customer care centre and negative word of mouth? You have to understand all that well enough before picking your objective function.

Finally, the key benefits of using machine learning, is that it can try vast amount of business logic cases till it reaches the optimum objective function, unlike the case of analysts in our space shuttle company who can only go so far with their rules. The machine learning approach is also automated in the sense that it keeps updating the business logic whenever new data arrives. Those two aspects make it scalable, more accurate and adaptable to changes.

Different kinds of learners

In this book we are going to cover two main paradigms of machine learning; the supervised learning and the unsupervised learning. Let's use our fictional Space Shuttle Corporation again to explain the difference between the two paradigms.

Supervised learning

Remember those old good days at school when you were given examples to practice on, along with their correct answers at the end to make sure you are solving them correctly. Then at the exam time, you are on your own. That's basically what supervised learning is. Say our fictional space vehicles company wants to predict if travelers will return their space vehicles back or not. Luckily, the company have had many travelers in the past, and they know already who of them returned back and who did not. Think of this data as a spread sheet, where each column has some information about the travelers: their financial statement, the number of kids they have, whether they are humans or aliens and their age, in Neptunian years of course. We call these columns features. There is one extra column for previous travelers that states whether they returned back or not, we call the column the label or target column. In the learning phase, we build a model using the features and the targets as well. Then for new travelers, we only know their features, thus we use our model to predict the targets. As you can see, the presence of the target in our historic is what makes it supervised learning.

Supervised learning is subdivided further into classification and regression. In the case where we have few predefined labels to predict we use a classifier for that. For example; "return" vs "not return" or human vs Martian vs Venusian. While if what we want to predict is a wide range numbers, say, how many years a traveler will take till they come back, then it is a regression problem, since these values can be 1 year, can be 3 years, can be 2 years 3 months and 7 days. It can be almost anything.

Due to their differences, the metrics we use to evaluate the classifiers are usually different from ones used with regression.

If we are using a classifier to tell whether a traveler is going to return or not. Then, of those travelers the classifier predicted to return, we care to measure what percentage of them did actually return. We call this score precision. Also, of all travelers who did return, we want to measure what percentage of them the classifier correctly predicted them to return. We call this one recall. Precession and recall can be calculated for each class, i.e. we can also calculate precision and recall for the travelers who did not return. Accuracy is another commonly used, and sometimes abused, measure. For each case in our historic data we know if a traveller actually returned or not (actuals) and we can also get our (predictions) of whether they will return or not. The accuracy calculates what percentage of the cases the class predictions and the actuals matches. As you can see, it is label agnostic, thus it can be misleading sometimes when the classes are highly imbalanced. Say in our business, 99% of our travelers actually return, and we build a dummy classifiers that predicts every single traveler to return, it will be accurate 99% of the time. This 99% accuracy is misleading, especially if you know the recall for the non-returning travelers is 0%. As we are going to see later on in this book, each measure has its pros and cons, and a measure is as good as how close it is to our business objectives. We will also going to learn about other metrics, such as F1 Score, AUC, Log Loss, etc.

If we are using a regressor to tell how long a traveler will stay. Then we care to tell how far are the numbers the regression is predicting from the reality. Say for 3 users the regressor expected them to stay 6, 9 and 20 years, while they actually stayed for 5, 10 and 26 years. One idea is to calculate the average of the differences between the predictions and the reality. The average of 6 - 5, 9 - 10 and 20 - 25, i.e. the average of 1, -1 and -6 is -2. One problem with this calculations that 1 and -1 cancelled each other. If you think about it, both 1 and -1 are mistakes the model made, so the sign might not matter much here. Thus, we have Mean Absolute Error (MAE) instead. It calculates the average of the absolute values of the differences, i,e, the average of 1, 1 and 6 is -2.67. It makes more sense now, but what if we can tolerate one year difference way more than 6 years difference? We can then use Mean Squared Error (MSE) to calculate the average of the differences squared, i.e. the average of 1, 1 and 36 is 12.67. Clearly each measure has it pros and cons here as well. Additionally, we can also use Median Absolute Error, or we can use maximum to end up with Max Error. Furthermore, sometimes your business objective can dictate other measures, say we want to penalize the model if it predicted a traveller to arrive one year later twice as much as when it predicted them to arrive one year earlier, what metric would you come up with then?

In practice, the lines between classification and regression problems can get blurred sometimes. I mean, for the case of how many years a traveler will take, you can still decide to bucket the range into 1-to-5, 5-to-10, and 10+ years. Then you end up with a classification problem to solve instead. Conversely, classifiers also return a probability along with their predicted targets. For the case of whether a user will return or not, a returned 60% and 95% means the same thing from a binary classifier's point of view, but the classifier is more confident that the second case will return more than the first case. Although this is still a classification problem, we may use Brier Score to evaluate our classifier here, which is actually Mean Squared Error in disguise. Most of time is it clear whether you are facing a classification or regression problem, but always keep your eyes open to the possibility of reformulating your problem if needed.  

Unsupervised learning

Life doesn't always provide us with correct answers like we used to have in school. We have been told that space travelers like it when they traveling are with like minded people. We already know a lot about our travelers, but of course no travel come to say, by the way I am a type A, B or C traveller. Thus, to be able to put our clients into groups we use a form of unsupervised learning classed clustering. It tries to come up with groups and put our travelers into them without us telling it what groups are there. Unsupervised learning lack targets, but this doesn't mean that we cannot evaluate our clustering algorithms still. For members of a cluster, we want them to be similar to each other, but we also want them to be dissimilar from members of the adjacent clusters. The silhouette coefficient basically measures that. We might come across other measures such as Davies-Bouldin Index and Calinski-Harabasz Index later in the book

Reinforcement learning

Reinforcement learning is out of the scope of this book and is not implemented in Scikit-learn. Nevertheless, I find it too cool to not briefly talk about here. In the supervised learning examples, we treated each traveller separately. When we wanted to know which travelers are going to return their space vehicles the earliest, our aim then was to pick the best travelers for our business. But if you think about it, the behavior of one traveller affects the experience of the others as well. We only allow space vehicles to stay up to 20 years in space. But we never explored the effect of allowing some travelers to stay longer, or the effect of having more strict rent period on the other travelers. And reinforcement learning is the answer to that, where the key to it is exploration and exploitation. Rather than dealing with each action separately, we may want to explore some suboptimal actions sometimes in order to reach an optimum overall set of actions. Reinforcement learning is used in robotics, where a robot has a goal and it can only reach it via sequence of steps; 2 steps to the right, 5 steps forward, etc. Nothing can tell whether a right vs left step is better on its own, it is only the sequence that has to be found to reach the best outcome. It is also commonly used in gaming, and in recommendation engines. If Netflix only served a user what matches their taste the best, one user may end up with nothing by Star Wars movies on their home screen. Reinforcement learning is needed then to explore less optimum matches for the sake of the user's overall experience.

Model development lifecycle

Problem undestanding

"All models are wrong, but some are useful", George Box

The first thing to do when developing a model is to understand the problem you are trying to solve very well. Not only what problem you are solving, but also why you are solving it, what impact are you expecting to have, what is the currently available solution that you are comparing your new solution to. I assume when Box said that all models are wrong, he meant that a model is an approximation to the reality by modeling one or few angles of it. And by understanding the problem you are trying to solve well, you are able to decide what angles of the reality you need to model the best.

You also need to understand the problem well to decide how to split your data for training and evaluation; more on that in the next section. You will also be able to decide what what of model to use. Is the problem suitable for supervised or unsupervised learning. Are we better off using classification or regression algorithms for that problem? What kind of classification algorithm will serve us best? Is a linear model good enough to approximate our reality? Do we need the most accurate model or one we can explain easily to third parties?

Finally, we need to understand what we are comparing our model too. What is the current baseline that we need to improve on? If there are already business rules in place, then our model have to be better at solving the problem at hand than these rules. And to be able to decide how better it is at solving the problem, we need evaluation metrics for that. Metrics that are suitable for our model and also as close as possible to our business requirements. If our aim is to increase revenue, then our metric should be good at estimating the increase in revenue if our model is used compared to the current status quo. If our aim is to increase repeated purchases regardless of the revenue, then different metrics may be more suitable then.


Splitting our data

As we have see in supervised learning, we train our model on a set of data where the correct answers (labels) are given. Learning, however, is only half of the problem. We also want to be able to tell whether the model we built is going to do a good job when used with future data or not. We cannot foresee the future, but we can use the data we already have now to also evaluate the model.  We do this but splitting our data into parts, we use one part of the data to train the model (training set) and use a separate part to evaluate the model (test set). Since we want our test set to be as close as possible to the future data, there are two key concepts to keep in mind

- How to best split the data?
- How to make sure training and test dataset are separate?

How to best split the data?

Say your users data is sorted by their country in alphabetical order. If you just take the first N records for training and the rest for testing, you will end up training on users from certain countries, and never learn anything about users from Zambia and Zimbabwe. Thus, one common solution is to randomize your data before splitting it. Random split is not always the best option though. Say we want to build a model to predict the stock prices, or the climate few years ahead. To be confident that our system will capture those temporal trends such as global warming, we better split our data based on time as well. We train on earlier data and see if the model can do a good job in predicting later data. Some other times we are predicting rare incidents. It can be that the number of fraud cases in your transaction data is 0.1%. If you randomly split your data, you may be unlucky and have the vast majority of the fraud cases in the training data and very few cases in the test data, or vice versa. Thus, it is advised to use stratification with such highly unbalanced data to make sure the distribution of your targets is more or less the same in both training and test data.

How to make sure training and test dataset are separate?

One of the most common mistake new data scientists may fall prey to is the look ahead bias. We use the test dataset to simulate the data we shall see in future, but sometimes the test dataset may contain information that we can only know once time passes. Take the case of the space vehicles, we may have two columns, one saying whether the vehicle returns back or on, and the other says how long the vehicle will take to return. If we are to build a classifier to predict whether a vehicle will return, we will use the former as our target, but we shall never use the latter as a feature. We can only know how long a vehicle stayed in the outer space once it is actually back. This example looks trivial, but believe me the look ahead bias is a common mistake especially with less obvious examples.

Besides training, you also learn things from data in order to preprocess it. Say, instead of user height in centimeters you want have a feature telling if a user height is above or below median. To do that, you need to go through the data and calculate the median. Since anything we learn we have to learn from the training set only, we also need to learn this median from the training set and not the entire data. Luckily, in all models and preprocessors in Scikit-learn there are separate methods for fit, predict and transform. This makes sure that anything learned from the data (via the fit method) is only learned from the training dataset only, and then can be applied to the test set (via predict and/or transform methods).

Development Set

When developing a model, we need to try multiple configurations of the model to decide which configuration gives the best results. To be able to do so, we usually further splits the training dataset into training and development sets. We try different configuration and train on the new training set, then measure the model's performance on the development set. Once we reach the best configuration, we then evaluate our final model on the test set. In the second chapter we will do all this in practice. By the way, I will be using the terms configurations and hyper-parameters interchangeably.

Evaluate our model

Evaluating your model performance is essential in picking the best algorithm for the job, and to be able to estimate how your model will perform in real life. As Box said, a model that is wrong can still be useful. Take this web startup. They run an ad campaign where they pay $ 1 per view, and they know that for each 100 viewers only one joins and buys stuff for $ 50. In other words they have to spend $ 100 to make $ 50. Obviously, that's a bad return of investment (ROI) for their business. Now if you create a model for them that can pick users for them to target with their ads, but your new model is only correct 10% of the time. Is 10% precision good or bad then? Well, of course, this model is wrong 90% of the time, which may sound like a very bad model, but if we calculate ROI now, then for every $ 100 they spend, they will make $ 500. I would definitely pay you to build me that model that is damn wrong, yet damn useful.

Scikit-learn provides a big number of evaluation metrics that we will be using the evaluate the models we build in this book. But remember, a metric is only useful if you really understand the problem you are solving and its business impact.

Deploy in production and monitoring

The main reason for many data scientist to use Python for machine learning, instead of R for example, is that it is easier to productionize your code. The python language has plenty of web frameworks to build API's with and put the machine lear behind. It is also supported by all cloud providers. I find it important that the team developing a model is also responsible for deploying it in production. Building your model in one language, then asking another team to port it into another language is error prone. Of course, having one person or team building and deploying the models may not be feasible due to scalability requirements or other implementation constraints, but keeping the two teams in close contact and making sure the ones developing the model can still understand the production code is essential and helps minimizing errors due to development and production code inconsistency.

We try our best not to have any look ahead bias when training our models. We hope data doesn't change after our models are training. We wish our code is bugs free. But we cannot guarantee any of these. We may overlook the fact that the user's credit score is only added to the database after they made their first purchase. We may not know that our developers decided to use the metric system to specify our inventory's weights while it was saved in pounds when the model was trained. And because of that, is it important to log all the predictions your model makes to be able to monitor it's preference in real life, and compare it to the test set's performance. You may also log the test set's performance every time you retrain the model, or keep track of the target's distribution over time.

Introduction to Scikit-Learn

Since you got this book already, then you probably don't need me to convince you why machine learning is important. But you may still have doubts why scikit-learn in particular. You may encounter names like TensorFlow, PyTorch and Spark more often during your daily news consumption that Scikit-learn.


Plays well with the Python data ecosystem

Scikit-learn is a Python toolkit built on top of NumPy, SciPy and Matplotlib. These choices means that it fits well into your daily data pipeline. As a data scientist, Python is most likely your language of choice since it t is good for both offline analysis and real-time implementation. You will also be using tools like Pandas to load data from your database, which also allows you do perform vast amount of transformation to your data. Since both Pandas and Scikit-learn are built on top of NumPy they play very well with each other. Matplotlib is the de facto data visualization tool for Python, which means you can use complex data visualizations to further understand your data and your model's performance.

As an open source code, that is heavily used in the community, it is very common to see other tools using the a very similar interface to scikit-learn. For example, Scikit-image is a library for image processing, while categorical-encoding and imbalanced-learn are separate libraries for data pre-processing. When we are going to use those tools in this book, you will notice how easily it is to integrate those libraries into your workflow when using Scikit-learn

Being a key player in the Python data ecosystem is what makes Scikit-learn the de-facto toolset for machine learning. This is the tool you will most likely hand your job application assignment in, use in solving Kaggle competition, and you will use it to solve most of your professional day to day machine learning problems at your job.


High-enough-level implementations of algorithms

Scikit-Learn implements a vast amount of machine learning, data processing and model selection algorithms. These implementations are abstract enough, so you only need to do minor changes when switching from one algorithm to the other. This is a key feature since you will need to quickly iterate between different algorithms when developing model to pick the best one for your problem. Having that said, this abstraction doesn't shield you from the algorithms configurations so you are still in control of your hyper-parameters settings.

When not to use Scikit-learn?

Most probably the reason would be a combination of deep learning or scale. Scikit-Learn's implementation of neural networks is limited. It does not provide the flexibility of TensorFlow or PyTorch if you want to have your custom architecture, and it does not support GPU's. All Scikit-learn's implementations run in memory on a single machine. I'd say that way more than 90% of the business are at a scale that is okay with such constraints. They can still fit their data in memory in big enough machines, thanks to cloud computing. They can cleverly engineer workarounds to deal with scaling issues, but if this limitations are not a thing they can deal with anymore, than they need some other tool for their problem.,


Introduction to Pandas

http://localhost:8888/notebooks/chapters/ch01/Pandas.ipynb



Book organization

The first two parts of the book will cover the supervised machine learning algorithms. First part will cover the basics, then will move to more advanced topics in the second part. Then the third and final part will cover the unsupervised learning, and topics such as anomaly detection and recommendation engines.

To keep this book as a practical guide, I made sure to provide examples within each chapter. I also did no want to separate the data preparation from the model creation. Although, topics such as data splitting, feature selection, data scaling and other data preparation and model evaluation topics and key concepts to know, we usually deal with them as part of the whole solution. I also feel those concepts are best understood in their right context. That's why within each chapters, I will be covering one main algorithm, but will use the examples there to shed the light on some of those concepts along the way. Thus, it is up to you to read this book from cover to cover, or use it as a reference and jump to the algorithms you need to know about when you need them, but then I advise you to skim through the other chapters even if the algorithm covered there you already know or don't need at the moment.



---


conda create -n scikitbook python=3.8 numpy scipy cython joblib pytest conda-forge::compilers conda-forge::llvm-openmp
